{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Speech Production and Models \n",
    "#### EQ2321 Speech and Audio Signal Processing\n",
    "\n",
    "Max. grade: 28.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions for the deliverables**\n",
    "\n",
    "Perform all (or as many as you can) of the tasks in this project assignment.\n",
    "Summarize your results in a presentation style document. \n",
    "You are going to present your work to the course responsible or one of the teaching assistants (TA)\n",
    "and it pays off to have a clear, well structured and brief document. Explicitly\n",
    "state, on the slides, the problem numbers.\n",
    "Include an algorithmic solution and/or numerical result and/or a graph illustrating your results.\n",
    "Your Matlab code should be functioning and easy to read.\n",
    "\n",
    "You have to send us you `Python` source code at least a day before the scheduled time for your\n",
    "presentation. \n",
    "Use a naming convention for your files that is easy to understand and associate with a specific problem number. \n",
    "Be prepared to demo your own code on a personal or our computer.\n",
    "Archive your files before sending them to us!\n",
    "\n",
    "**Grading system**\n",
    "Each correctly solved task brings you a certain amount of points.\n",
    "The grade you get depends on the amount of points you accumulate and your ability to motivate your solutions during the presentation.\n",
    "A passing grade corresponds to having 30\\% and more of the total amount of points, while an excellent grade requires 85\\% and more.\n",
    "No points will be granted for a correct response if you cannot motivate your answer.\n",
    "\n",
    "Keep in mind, when working in a group of two, that both people are expected to be able to make a complete presentation and answer questions related to all problems.\n",
    "You may be graded differently depending on your responses.\n",
    "The grade you get depends on the amount of correctly solved tasks and your ability to motivate the solutions during the presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The first assignment is perhaps the most important for two reasons: \n",
    "Firstly, the knowledge obtained in this exercise is the basis for the whole course, and the program code that you produce will be useful in coming exercises. \n",
    "Secondly, this exercise is relatively easy, and this is a chance for all who are not familiar with the `Python` environment to catch up. \n",
    "\n",
    "The coming computer exercises will require more `Python` knowledge (but will also be proportionally more\n",
    "exciting).\n",
    "On the course homepage you can find the speech data which you will analyze in this computer assignment.\n",
    "Download and unzip the file assignment1.zip from the course webpage. \n",
    "Store it in a directory which is readable from the `Python` prompt.\n",
    "\n",
    "All the signals are sampled at 8 kHz. The signals are stored in double floating point format, but they originate from 16 bit AD conversion.\n",
    "\n",
    "In this first assignment a lot of code is provided to you.\n",
    "Note that this is for your convenience only; if you prefer to write your own code from scratch we\n",
    "encourage you to do so, but it may be time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandwidth of Speech (2.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first experiment we study how low-pass filtering affects the speech signal. Load the files male44.waw and female44.wav. The speech is sampled at 44.1 kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "male_fname=\"male44.wav\"\n",
    "female_fname=\"female44.wav\"\n",
    "assignment_file=\"assignment1.pkl\"\n",
    "\n",
    "def read_assignment_file(fname):\n",
    "    with open(fname,\"rb\") as f:\n",
    "        d=pkl.load(f)\n",
    "    return d\n",
    "\n",
    "\n",
    "d=read_assignment_file(assignment_file)\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "\n",
    "def read_wav(fname):\n",
    "    raw_data=wavfile.read(fname)\n",
    "    return raw_data[1].astype(np.float32), raw_data[0]\n",
    "\n",
    "\n",
    "# https://python-sounddevice.readthedocs.io/en/0.3.6/\n",
    "# !pip install sounddevice\n",
    "\n",
    "import sounddevice as sd\n",
    "\n",
    "def play_wav(x,fs):\n",
    "    sd.play(x.astype(np.int16),fs)\n",
    "\n",
    "\n",
    "from scipy.signal import firwin,filtfilt\n",
    "def lowpass(x,fc,fs=44100):\n",
    "    \"\"\" x is the signal, \n",
    "        fc is the cuttoff frequency expressed in Hz, must be between [0,fs/2]\n",
    "    \"\"\"\n",
    "    b = firwin(1024, fc,fs=fs)\n",
    "    y = filtfilt(b,1,x)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (0.5 pts)\n",
    "\n",
    "\n",
    "This is a test of intelligibility, i.e., how well we can understand what is spoken.\n",
    "\n",
    "At what cut-off frequency do you start hearing what is said?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "<li style=\"color: blue\">For male voice: 1500 Hz</li>\n",
    "<li style=\"color: blue\">For female voice: 2000 Hz</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 44100\n",
    "xmale,fs=read_wav(male_fname)\n",
    "xfemale,fs=read_wav(female_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_wav(lowpass(xmale,1500,fs=fs),fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_wav(lowpass(xfemale,2000,fs=fs),fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (0.5 pts)\n",
    "\n",
    "At what cut-off frequency do you start to hear a degradation in overall speech quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "<p style=\"color: blue\">Starting from a high cut-off frequency (fs/2), we found an overall degradation in speech quality at:</p>\n",
    "<li style=\"color: blue\">male voice: 2000 Hz</li>\n",
    "<li style=\"color: blue\">female voice: 2500 Hz</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_wav(lowpass(xmale,2000,fs=fs),fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_wav(lowpass(xfemale,2500,fs=fs),fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (1 pts)\n",
    "\n",
    "Why, do you think (you are allowed to speculate!), is a sampling frequency of 8 kHz used in plain old telephony services (POTS)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "<p>Because is a good balance between bandwidth used and quality of speech. Seeing that the speech degrades at around 2-2.5 kHz, we would need at least 5 kHz as a sampling frequency, and 8kHz is well beyond that mark.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voiced and Unvoiced Speech Sounds (8 pts)\n",
    "\n",
    "From a time plot, find voiced and unvoiced regions. Study the harmonic structure and envelope of the speech spectra corresponding to voiced and\n",
    "unvoiced regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy.signal.windows import hann, boxcar\n",
    "from scipy.fftpack import fft, fftshift, ifft, fftfreq\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "plt.rcParams[\"figure.figsize\"] = (12,5)\n",
    "x = d[\"male_long\"]\n",
    "fs = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (0.5 pts)\n",
    "\n",
    "Create a time plot with voiced and unvoiced regions marked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a5e7ea9d3844d191f53cd1d3398c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.2, 1.45, -4500.0, 4500.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tStep = 1/fs\n",
    "finalT = x.size * tStep\n",
    "t = np.arange(0, finalT, tStep)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t, x)\n",
    "plt.axvline(1.2410, color='r')\n",
    "plt.axvline(1.3250, color='r')\n",
    "plt.axvline(1.3255, color='g')\n",
    "plt.axvline(1.4, color='g')\n",
    "plt.text(1.27, 4000, \"voiced\")\n",
    "plt.text(1.35, 4000, \"unvoiced\")\n",
    "plt.xlabel(\"t (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.axis([1.2, 1.45, -4500, 4500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (0.5 pts)\n",
    "\n",
    "In the time plot, mark the regions where the pitch is the highest and the lowest. What are the pitch frequencies in those regions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "<li style=\"color: blue\">For voiced region</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1fc8304ed94715aa2c0c5b7cb45af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Amplitude')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(t, x)\n",
    "plt.xlabel(\"t (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "#plt.axis([1.242, 1.29, -4500, 4500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li style=\"color: blue\">For unvoiced region:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e800d700303946efb78783e994f68b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Amplitude')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(t, x)\n",
    "plt.xlabel(\"t (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "#plt.axis([1.2, 1.45, -4500, 4500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (0.5 pts)\n",
    "\n",
    "Plot the DFT based spectrum for a voiced frame using, the following snippet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d792d99c524f868f84636b40ffade8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S= 9933#... # Start of the frame\n",
    "N=392#... # Length of the frame\n",
    "\n",
    "T = 1 / fs\n",
    "\n",
    "y = x[S:S+N].reshape(N,)\n",
    "yf = fft(y)\n",
    "\n",
    "w = hann(N)\n",
    "ywf = fft(y*w)\n",
    "xf = fftfreq(N, T)[:N//2]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "#plt.semilogy(xf[1:N//2], 10*np.log10(np.abs(yf[1:N//2])**2), '-b')\n",
    "plt.plot(xf[1:N//2], 10*np.log10(np.abs(ywf[1:N//2])**2), '-r', marker='o')\n",
    "plt.xlabel('Freq (Hz)')\n",
    "plt.ylabel('FFT Amplitude |X(freq)|')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-44.89169109309552-4.4857052564941675j)\n"
     ]
    }
   ],
   "source": [
    "print(ywf[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (1 pts)\n",
    "\n",
    "What is the mathematical expression of element `X[3]`, i.e.,\n",
    "the third element in the FFT vector? What analog frequency (between\n",
    "0-8000 Hz) does `X[3]` correspond to?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "$$ \\sum \\limits _{n=0} ^{N-1} e^{-6j\\pi \\frac{n}{N}} x[n] $$ \n",
    "\n",
    "It corresponds to f ~ 61 Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (0.5 pts)\n",
    "\n",
    "What is the fundamental frequency (pitch) in your particular case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "f ~ 180 Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (1 pts)\n",
    "\n",
    "What frame length is appropriate? What compromise do you have to\n",
    "make when choosing frame length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Frame length = 392 samples. If we choose a bigger frame, we start analyzing other sounds (might be voiced and/or unvoiced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 (1 pts)\n",
    "\n",
    "Replace the hanning window function with a rectangular window and compare the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd656e03867405bb322260847e6ee76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S= 9933#... # Start of the frame\n",
    "N=392#... # Length of the frame\n",
    "\n",
    "T = 1 / fs\n",
    "\n",
    "y = x[S:S+N].reshape(N,)\n",
    "yf = fft(y)\n",
    "\n",
    "w = boxcar(N)\n",
    "ywf2 = fft(y*w)\n",
    "xf = fftfreq(N, T)[:N//2]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xf[1:N//2], 10*np.log10(np.abs(ywf[1:N//2])**2), '-b')\n",
    "plt.plot(xf[1:N//2], 10*np.log10(np.abs(ywf2[1:N//2])**2), '-r')\n",
    "plt.xlabel('Freq (Hz)')\n",
    "plt.ylabel('FFT Amplitude |X(freq)|')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peaks are more defined (narrow frequencies) for the Hanning window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 (0.5 pts)\n",
    "\n",
    "Extend the code above with the following lines to plot also the LP envelope of the short-time speech spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\david\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\david\\anaconda3\\lib\\site-packages (from scipy) (1.19.2)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.signal' has no attribute 'correlation_lags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-43119bcfc251>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# M is the prediction order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_envelop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-43119bcfc251>\u001b[0m in \u001b[0;36mget_envelop\u001b[1;34m(xf, M, N, fs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_envelop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m44100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mlags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolve_toeplitz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-43119bcfc251>\u001b[0m in \u001b[0;36mxcorr\u001b[1;34m(x, y, M)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m     14\u001b[0m     \u001b[0mcorr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrelate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"full\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mlags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrelation_lags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"full\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlags\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlags\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mcorr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy.signal' has no attribute 'correlation_lags'"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import solve_toeplitz, toeplitz\n",
    "!pip install scipy\n",
    "from scipy import signal\n",
    "def xcorr(x,y,M=10):\n",
    "    \"\"\"\n",
    "    Perform Cross-Correlation on x and y\n",
    "    x    : 1st signal\n",
    "    y    : 2nd signal\n",
    "\n",
    "    returns\n",
    "    lags : lags of correlation\n",
    "    corr : coefficients of correlation\n",
    "    \"\"\"\n",
    "    corr = signal.correlate(x, y, mode=\"full\")\n",
    "    lags = signal.correlation_lags(len(x), len(y), mode=\"full\")\n",
    "    idx=(lags>=-M) & (lags<=M)\n",
    "    corr = corr[idx]\n",
    "    lags=lags[idx]\n",
    "    return lags[len(lags)//2:], corr[len(lags)//2:]\n",
    "\n",
    "def get_envelop(xf,M,N=1024,fs=44100):\n",
    "\n",
    "    lags, r = xcorr(xf, xf,M=M)\n",
    "\n",
    "    a=np.concatenate([np.ones(1), solve_toeplitz(r[:-1], -r[1:])]).reshape(-1,1)\n",
    "    e=(a.T @ r)[0]\n",
    "    w, h2=signal.freqz(1,a,N,whole=True,fs=fs)\n",
    "\n",
    "    fig,ax=plt.subplots(1,1)\n",
    "\n",
    "    ax.plot(w[1:N//2+1],10*np.log10(np.abs(ywf[1:N//2+1])**2),label=\"$X(f)$\" \"-r\")\n",
    "    ax.plot(w[1:N//2+1],10*np.log10(e*np.abs(h2[1:N//2+1])**2),label=\"$Envelop$\")\n",
    "    ax.set_xlabel(\"Frequency (Hz)\")\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "    ax.legend()\n",
    "    return fig,ax\n",
    "\n",
    "\n",
    "# M is the prediction order\n",
    "M = 10\n",
    "fig,ax=get_envelop(xf,M,N=N,fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 (1 pts)\n",
    "\n",
    "What is the mathematical expression of element `c(3)`? Why do we multiply `abs(h[1:N//2+1])**2` by `e` before plotting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 (1 pts)\n",
    "\n",
    "What prediction order do you recommend? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 (0.5 pts)\n",
    "\n",
    "Repeat the spectrum plotting for an unvoiced frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3383f1101b8640e5a6376b38c56e8350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S= 9933#... # Start of the frame (CHANGE S to unvoiced frame)\n",
    "N=392#... # Length of the frame\n",
    "\n",
    "T = 1 / fs\n",
    "\n",
    "y = x[S:S+N].reshape(N,)\n",
    "yf = fft(y)\n",
    "\n",
    "w = boxcar(N)\n",
    "ywf = fft(y*w)\n",
    "xf = fftfreq(N, T)[:N//2]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "#plt.semilogy(xf[1:N//2], 10*np.log10(np.abs(yf[1:N//2])**2), '-b')\n",
    "plt.plot(xf[1:N//2], 10*np.log10(np.abs(ywf[1:N//2])**2), '-r')\n",
    "plt.xlabel('Freq (Hz)')\n",
    "plt.ylabel('FFT Amplitude |X(freq)|')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some remarks\n",
    "\n",
    "1. The statement “Plot the spectrum” is vague. There are many alternatives,\n",
    "e.g. plotting the real, and imaginary part of the FT separately. Here we\n",
    "focus on the squared magnitude of the DFT coefficients, and hence disre-\n",
    "gard the phase. Also it is common to plot the spectrum in a logarithmic\n",
    "scale, i.e., in decibels (dB), since this better reflects what we actually hear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formants (2.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the speech variable “male short”, try to identify the vowels by comparing\n",
    "the formants from an LP analysis to the formant frequencies of template vowels\n",
    "in Table 1. Note that the table contains only the three first formants; you can\n",
    "often see up to four formants.\n",
    "\n",
    "\n",
    "vowel|F1|F2|F3|example\n",
    "---:|:--:|:|--:|:---:|\n",
    "iy|270|2290|3010|beet\n",
    "ih|390|1990|2550|bit\n",
    "eh|530|1840|2480|bet\n",
    "ae|660|1720|2410|bat\n",
    "ah|520|1190|2390|but\n",
    "aa|730|1090|2240|hot\n",
    "ao|570|840|2410|bought\n",
    "uh|440|1020|2240|foot\n",
    "uw|300|870|2240|boot\n",
    "er|490|1350|1690|bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (1 pts)\n",
    "\n",
    "In the time plot from the previous section, add the formant frequencies\n",
    "and the vowels you come up with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (1.5 pts)\n",
    "\n",
    "How are pitch and formant frequencies related?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemes and Allophones (2.5 pts)\n",
    "\n",
    "Vowels, which we studied in the previous section, constitute one class of phonemes.\n",
    "Here we identify what other phonemes are contained in “male short”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (1 pts)\n",
    "\n",
    "In the time plot from the previous sections, mark the regions of the other phonemes and label each region with the correct phonetic symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (1 pts)\n",
    "\n",
    "Can consonants have formant frequencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (0.5 pts)\n",
    "\n",
    "What is a diphthong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (0.5 pts)\n",
    "\n",
    "How many phonemes are there in English?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (0.5 pts)\n",
    "\n",
    "What is a phone? What is an allophone? How many allophones are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Spectrogram (3 pts)\n",
    "\n",
    "Program a spectrogram function in MATLAB. A spectrogram shows the energy in speech as a function of time and frequency. The result is often displayed with time on the horizontal axis, and frequency on the vertical\n",
    "axis. The energy (squared magnitude, and in dB) of the DFT coefficients are illustrated by e.g. a grayscale, where black corresponds to high energy, and white represents low energy. To obtain a smooth evolution over time,\n",
    "we want to use overlapping analysis windows, see Figure 1. \n",
    "Test your function on the speech samples provided for this lab. For the presentation have the result for “male short” available.\n",
    "<img src=\"./windowing.png\" alt=\"alt text\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (0.5 pts)\n",
    "\n",
    "First program a function that extracts overlapping frames and plots each frame. \n",
    "Input arguments should be update length, and analysis length.\n",
    "\n",
    "Call `myspectrogram(x_short, 256, 32)` and view the animation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myspectrogram(x, alen, ulen)\n",
    "    \"\"\" x is the speech vector, \n",
    "    alen is the analysis frame length, \n",
    "    ulen is the update length\"\"\"\n",
    "    N = len(x)\n",
    "    naf = np.floor((N-alen+ulen)/ulen)\n",
    "    n1 = 1\n",
    "    n2 = alen\n",
    "    for n in range(1,naf+1):\n",
    "        xf = x[n1:n2]\n",
    "        plt.figure\n",
    "        plt.plot(xf)\n",
    "        plt.axis([1 alen min(x) max(x)])\n",
    "        plt.pause(0.05)\n",
    "        n1 = n1 + ulen\n",
    "        n2 = n2 + ulen\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (0.5 pts)\n",
    "\n",
    "Next add frequency analysis of each frame. Use the code for the DFT based spectrum from section 3 (dont forget to window before DFT). If you want to plot both the time domain signal, and the spectrum in the same plot, but in different parts of the window, see subplot. Illustrate the function as in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (1 pts)\n",
    "\n",
    "Now we are ready to implement the classical spectrogram. Instead of plotting the squared magnitude of the DFT, you put it into the columns of a matrix S. Preallocate the matrix with \n",
    "```python\n",
    "S = np.zeros((alen/2+1, naf))\n",
    "```\n",
    "In the loop you write to column n by \n",
    "\n",
    "```python\n",
    "S[:,n] = \n",
    "```\n",
    "In this assignment it is important that the right hand side is a column vector. To plot the spectrogram, use the following code:\n",
    "```python\n",
    "plt.colormap(\"gray\") # The classical spectrogram is gray,\n",
    "\n",
    "# type help gray for other colormaps\n",
    "plt.imagesc(np.flipud(-S)); # flipud flips S along the frequency axis so that\n",
    "# frequencies increase when we move up the vertical axis\n",
    "# -S makes black correspond to high energy!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (0.5 pts)\n",
    "\n",
    "Produce a narrow-band spectrogram. Be prepared to indicate the funda-\n",
    "mental frequency track, and the boundaries of the phonemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (0.5 pts)\n",
    "\n",
    "Produce a wide-band spectrogram. Here the spectrogram tends to become blocky if smoothing is not performed in the frequency domain. Incorporate smoothing by zero-padding before applying the DFT. Can you see the\n",
    "formant trajectories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Parameter Estimation (5 pts)\n",
    "\n",
    "In this section, you will analyze (estimate) the parameters of a vocoder model. In the next section you will synthesize speech based on the estimated parameters. The parameters we will estimate are frame energy, pitch, vocal tract filter coefficients, and voiced/unvoiced classification. The estimation should be done on a frame-by-frame basis and we will use overlapping analysis frames. Just as in the case of the spectrogram the choice of analysis frame length is a compromise between having long enough frames to get reliable estimates, but not too long so that rapid events are averaged out 2. Also as in the spectrogram case, the update length controls the smoothness of\n",
    "the parameter trajectories over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (0.5 pts)\n",
    "\n",
    "Let us warm up with estimation of frame energy (normalized per sample)\n",
    "$E = \\frac{1}{N_\\alpha}\\sum_{n=0}^{N_\\alpha-1}x^2(n)$\n",
    "\n",
    "where $N_\\alpha$ is the analysis frame length and the sum is over the samples in one analysis frame. Write a function that returns the frame energies in a vector. Start with the function skeleton from the first task in section 6. Before the loop over all frames, allocate space for the vector of energies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(x, alen, ulen):\n",
    "    naf = ...\n",
    "    # Initialization\n",
    "    E = np.zeros((naf, 1));\n",
    "    # Inside loop\n",
    "    E[n] = ...\n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (1 pts)\n",
    "\n",
    "Next let us look at voiced/unvoiced detection. This is a difficult problem, but here we resolve to a simple solution based on zero-crossings. A zero\n",
    "crossing occurs when $x(n)x(n − 1) < 0$. By counting all those occurrences within a frame, and normalizing by the frame length, a (near) continuous\n",
    "parameter is obtained. To make a binary decision a threshold can be used (by normalizing with the frame length, the threshold becomes independent\n",
    "(almost) of the analysis frame length). Extend your analysis function like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(x, alen, ulen):\n",
    "    naf=...\n",
    "    # Initialization\n",
    "    E = np.zeros((naf, 1));\n",
    "    ZC = np.zeros((naf, 1));\n",
    "    V = np.zeros((naf, 1));\n",
    "    \n",
    "    # Inside loop\n",
    "    E[n] = ...\n",
    "    ZC[n] = ... # The normalized number of zero crossings\n",
    "    V[n] = ... # Equal to 1 if voiced, 0 if unvoiced.\n",
    "    return E, ZC, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (0.5 pts)\n",
    "\n",
    "Next extend your analysis function by incorporating code for vocal tract\n",
    "filter estimation via LP analysis. The code from Section 3 should work\n",
    "fine! Store the filter parameters in the rows of a matrix like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(x, alen, ulen, M):\n",
    "    # Initialization\n",
    "    E = np.zeros((naf, 1));\n",
    "    ZC = np.zeros((naf, 1));\n",
    "    V = np.zeros((naf, 1));\n",
    "    A = np.zeros((naf, M+1)); # M is the prediction order.\n",
    "    # M+1 allows space for the leading 1\n",
    "    # Inside loop\n",
    "    E[n] = ...\n",
    "    ZC[n] = ... # The normalized number of zero crossings\n",
    "    V[n] = ... # Equal to 1 if voiced, 0 if unvoiced.\n",
    "    A[n,:] = ... # Make sure the polynomial coefficients are in a row vector\n",
    "    return E, ZC, V, A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (0.5 pts)\n",
    "\n",
    "Finally extend the function with pitch analysis. Many speech coding (compression) systems depend on accurate pitch analysis. Here we base our estimation on the correlation function of the frame. You may choose if you\n",
    "wish to use the ACF or the normalized cross-correlation function (slightly more difficult to program). The problem you have to solve is how to find the lag of the peak in the ACF that corresponds to one pitch period. This is usually easy to do “manually”, i.e., by looking at the ACF, but you have to make the computer do it automatically! The function will finally look something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(x, alen, ulen, M):\n",
    "    # Initialization\n",
    "    E = np.zeros((naf, 1));\n",
    "    ZC = np.zeros((naf, 1));\n",
    "    V = np.zeros((naf, 1));\n",
    "    A = np.zeros((naf, M+1)); # M is the prediction order.\n",
    "    # M+1 allows space for the leading 1\n",
    "    P = np.zeros((naf, 1));\n",
    "    # Inside loop\n",
    "    E[n] = ...\n",
    "    ZC[n] = ... # The normalized number of zero crossings\n",
    "    V[n] = ... # Equal to 1 if voiced, 0 if unvoiced.\n",
    "    A[n,:] = ... # Make sure the polynomial coefficients are in a row vector!\n",
    "    P[n] = ... # Pitch period in samples \n",
    "    return E, ZC, V, A, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (0.5 pts)\n",
    "\n",
    "Plot the output of your analysis function with the following code (or include the code below in the analysis function).\n",
    "Test and tune your analysis function on the files “male long”, and “female\n",
    "long”. Be prepared to provide plots with smooth temporal evolution, i.e., with ulen = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "plt.plot(x) # Plot the input waveform\n",
    "plt.axis([1 len(x) np.min(x) np.max(x)]);\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "plt.plot(np.sqrt(E)) # Plot the standard deviation\n",
    "plt.axis([1 len(E) np.min(np.sqrt(E)) np.max(np.sqrt(E))]);\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "plt.plot(V, .) # Plot voiced/unvoiced decision\n",
    "plt.axis([1 len(V) 0 1]);\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "plt.plot(ZC) # Plot the normalized number of zero-crossings\n",
    "plt.axis([1 len(ZC) np.min(ZC) np.max(ZC)]);\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "F = 8000./P;\n",
    "plt.plot(F) # Plot the fundamental frequency in Hz\n",
    "plt.axis([1 len(F) 0 600]);\n",
    "\n",
    "plt.subplot(3,2,6)\n",
    "S = np.zeros(512, naf);\n",
    "for n in range(1,naf+1):\n",
    "    S[:,n] = 20*np.log10(np.abs(signal.freqz(1,A[n,:],512)));\n",
    "\n",
    "S = np.flipud(S);\n",
    "plt.colormap(gray);\n",
    "plt.imagesc(S); # Illustrate the vocal tract envelope in a spectrogram style!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (1 pts)\n",
    "\n",
    "What could make the voiced/unvoiced detection go wrong? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 (1 pts)\n",
    "\n",
    "In the pitch estimation, what can cause pitch doubling? What can be the reason for pitch halving?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocoder (5 pts)\n",
    "\n",
    "In this section we will synthesize speech based on the parameters from the analysis. Speech coding (compression) systems based on this principle are called vocoders and can reach bit rates as low as 2 kbits/second when the parameters 12are quantized. This can be compared with 64 kbits/second for the PCM system used in fixed telephone networks. The quality of the vocoder is lower though. Here we will synthesize speech based on unquantized parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (1 pts)\n",
    "\n",
    "Let us start by only incorporating the vocal tract filter in the synthesis.\n",
    "Make sure you use the same update length (ulen) in the synthesis as in the analysis! Make sure you understand why ulen samples are generated in the synthesis for each analysis frame 3.\n",
    "Test with a pulse train excitation (use a constant pitch corresponding to 100 Hz). Then test with noise excitation. Can you hear what is said based on only vocal tract parameters (you may need to rescale the whole output\n",
    "before playing back)?\n",
    "\n",
    "\n",
    "Note:\n",
    "Instead of varifilter you can also use the MATLAB function filter to implement the\n",
    "synthesis. But then you have to make sure that the final filter state after one frame is used as\n",
    "initial state in the next frame. Unfortunately, the filter structure in MATLAB is not designed\n",
    "for time-varying coefficients and audible clicks may result. To remedy this, see the function\n",
    "filtic, and varifilter. Another option is to implement the filter yourself. In that way you\n",
    "have full control over the filter memory. The downside is that your filter will probably execute\n",
    "slower than MATLAB’s filter which is a built-in function.\n",
    "\n",
    "Write a function like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesis1(E, ZC, V, A, P, ulen):\n",
    "    # We have included all the estimated parameters as input arguments but here we only use A!\n",
    "    n_frames = A.shape[1]; # Assuming filter coefficients are stored row-wise\n",
    "    \n",
    "    # Create a pulse train excitation:\n",
    "    cp = ...; # Constant pitch period in samples\n",
    "    pexc = np.zeros((n_frames*ulen, 1));\n",
    "    pexc[1:cp:end] = 1;\n",
    "    \n",
    "    # Create noise excitation:\n",
    "    nexc = ...\n",
    "    n1 = 1\n",
    "    n2 = ulen\n",
    "    Z = []\n",
    "    s = np.zeros((n_frames*ulen,1))\n",
    "    for n in range(1, n_frames+1):\n",
    "        # Filter the excitation through the production (vocal tract) filter:\n",
    "        s[n1:n2], Z = signal.lfiltic(1, A[n,:], pex[n1:n2], x=Z)\n",
    "        n1 = n1+ulen\n",
    "        n2 = n2+ulen\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (1 pts)\n",
    "\n",
    " Make sure the energy contour of the synthesized speech matches that of the original speech. A crude but simple way is to (re)normalize each synthesized frame to have frame energy $E(n)$.\n",
    "\n",
    "Write a new function `synthesis2` (naturally based on `synthesis1`).\n",
    "Test with the pulse and noise excitations from the previous task. How much does the energy contour contribute to the intelligibility?\n",
    "\n",
    "Note: In the most straightforward energy normalization, a problem occurs when the synthesis\n",
    "frame contains no pitch pulses (this will happen frequently if your update length is short).\n",
    "Can you suggest a fix? It may require estimating the prediction residual energy (thus changing\n",
    "also your analysis function slightly), and re-normalizing the excitation (before filtering) during\n",
    "synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesis2(E, ZC, V, A, P, ulen):\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (0.5 pts)\n",
    "\n",
    "Switch between pulse excitation and noise excitation based on the voiced/\n",
    "unvoiced decision, and write `synthesis3`.\n",
    "Does this increase intelligibility? Our vocoder model assumes that all speech sounds are either voiced or unvoiced. Is this true? Discuss how our vocoder could be modified?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesis3(E, ZC, V, A, P, ulen):\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (1 pts)\n",
    "\n",
    " Create a function `synthesis4` by adding time varying pitch! Now you cannot use the variable pexc anymore, but have to create the pulse train \"on the fly\".\n",
    "One way is to keep a counter that is incremented for each sample during voiced frames.\n",
    "When the counter equals or exceeds the current pitch value, a pulse is inserted into the excitation, and the counter is set to zero. The quality of the synthesized speech depends a lot on the quality of the pitch estimate.\n",
    "Make sure the pitch contour is smooth in voiced regions.\n",
    "If necessary you may median filter the pitch contour prior to synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesis4(E, ZC, V, A, P, ulen):\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (1.5 pts)\n",
    "\n",
    "Tune your analysis and synthesis functions so that the produced speech has as high quality as possible. We want to hear an exciting result! Please\n",
    "make sure the vocoder output is readily available in a separate file when you come to present your results. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b98e5309e6e3b0bade2ed4a1aa225e8ff7275f11bed2b4c0572310c8cf94ab4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
